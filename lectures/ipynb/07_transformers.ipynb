{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc4be1b",
   "metadata": {},
   "source": [
    "# Intro to Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59655b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  100 | loss 0.9548\n",
      "step  200 | loss 0.1717\n",
      "step  300 | loss 0.0666\n",
      "step  400 | loss 0.0409\n",
      "step  500 | loss 0.0286\n",
      "\n",
      "--- generated ---\n",
      "hello world! my name is eeko and I am a pitbull mix. I love to play fetch and cuddle. nice to meet youdleto nice me. e. meeeeko y niceeevenanand t fend ceko c niceee. ce. ple cee. feeeeeeeeeko and ceko ndle\n"
     ]
    }
   ],
   "source": [
    "# Big idea:\n",
    "#   - We convert text -> token IDs\n",
    "#   - We train a Transformer to predict the next token at every position\n",
    "#   - We use a causal mask so a position cannot attend to future tokens\n",
    "#   - We can then generate new text autoregressively\n",
    "\n",
    "import math         \n",
    "import torch              \n",
    "import torch.nn as nn                \n",
    "import torch.nn.functional as F      \n",
    "\n",
    "# ----------------------------\n",
    "# 1) Tiny \"tokenizer\" (character-level)\n",
    "# ----------------------------\n",
    "\n",
    "text = \"hello world! my name is eeko and I am a pitbull mix. I love to play fetch and cuddle. nice to meet you! caleb is cool.\" # create a tiny corpus by repeating a short string many times\n",
    "chars = sorted(list(set(text)))         # unique characters in the corpus, sorted for consistent ordering\n",
    "vocab_size = len(chars)                 # number of unique characters (vocabulary size)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # map each character -> integer ID\n",
    "itos = {i: ch for ch, i in stoi.items()}      # inverse map: integer ID -> character\n",
    "\n",
    "def encode(s):\n",
    "    # convert a string into a 1D tensor of token IDs\n",
    "    return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "\n",
    "def decode(ids):\n",
    "    # convert a sequence of token IDs back into a Python string\n",
    "    return \"\".join([itos[int(i)] for i in ids])\n",
    "\n",
    "data = encode(text)                      # the entire corpus as a 1D tensor of IDs, shape (T,)\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Make training batches (x -> y shifted by 1)\n",
    "# ----------------------------\n",
    "\n",
    "def get_batch(data, batch_size, block_size, device):\n",
    "    # Choose random starting indices so each batch is different\n",
    "    ix = torch.randint(0, len(data) - block_size - 1, (batch_size,))  # (B,)\n",
    "\n",
    "    # Build input sequences x: tokens [i, ..., i+block_size-1]\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])               # (B, S)\n",
    "\n",
    "    # Build target sequences y: the \"next token\" for each position\n",
    "    # y is the same as x but shifted by 1 position in the original text\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])           # (B, S)\n",
    "\n",
    "    # Move tensors to the chosen device (CPU/GPU)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Causal attention mask\n",
    "# ----------------------------\n",
    "\n",
    "def causal_mask(seq_len, device):\n",
    "    # Create an upper-triangular matrix of True values above the diagonal\n",
    "    # True means \"mask out\" (disallow attention to that position)\n",
    "    #\n",
    "    # Example for seq_len=4:\n",
    "    # [[False, True,  True,  True],\n",
    "    #  [False, False, True,  True],\n",
    "    #  [False, False, False, True],\n",
    "    #  [False, False, False, False]]\n",
    "    #\n",
    "    # This ensures token at position i can only attend to <= i (no future tokens).\n",
    "    return torch.triu(torch.ones(seq_len, seq_len, device=device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Tiny GPT-like language model (Transformer encoder blocks + causal mask)\n",
    "# ----------------------------\n",
    "\n",
    "class TinyTransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=2, block_size=64, dropout=0.1):\n",
    "        super().__init__()                                   # initialize nn.Module internals\n",
    "\n",
    "        self.vocab_size = vocab_size                         # store vocab size (number of possible tokens)\n",
    "        self.d_model = d_model                               # store embedding dimension\n",
    "        self.block_size = block_size                         # maximum sequence length this model supports\n",
    "\n",
    "        self.tok_emb = nn.Embedding(vocab_size, d_model)     # token embedding lookup table: (vocab -> d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)     # positional embeddings: (position -> d_model)\n",
    "\n",
    "        # Define a single Transformer encoder block\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,                                 # embedding dimension\n",
    "            nhead=nhead,                                     # number of attention heads\n",
    "            dim_feedforward=4*d_model,                       # hidden size in the MLP inside the block\n",
    "            dropout=dropout,                                 # dropout probability\n",
    "            activation=\"gelu\",                               # activation in the block MLP\n",
    "            batch_first=True,                                # use tensor shape (B, S, E) instead of (S, B, E)\n",
    "            norm_first=True                                  # pre-LayerNorm style (common in modern Transformers)\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(            # stack multiple encoder blocks\n",
    "            enc_layer,                                       # the block definition\n",
    "            num_layers=num_layers                            # how many blocks to stack\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)                    # final LayerNorm before the output head\n",
    "        self.head = nn.Linear(d_model, vocab_size)           # map hidden states -> logits over vocabulary\n",
    "\n",
    "    def forward(self, idx):\n",
    "        # idx: (B, S) tensor of token IDs\n",
    "        B, S = idx.shape                                     # batch size and sequence length\n",
    "        assert S <= self.block_size                          # prevent sequences longer than positional embedding table\n",
    "\n",
    "        pos = torch.arange(S, device=idx.device)             # positions [0..S-1], shape (S,)\n",
    "        tok = self.tok_emb(idx)                              # token embeddings, shape (B, S, E)\n",
    "        pos = self.pos_emb(pos)                              # position embeddings, shape (S, E)\n",
    "        x = tok + pos                                        # broadcast add positions to each batch, shape (B, S, E)\n",
    "\n",
    "        attn_mask = causal_mask(S, idx.device)               # causal mask, shape (S, S)\n",
    "        x = self.transformer(x, mask=attn_mask)              # apply stacked Transformer blocks, shape (B, S, E)\n",
    "\n",
    "        x = self.ln_f(x)                                     # final normalization, shape (B, S, E)\n",
    "        logits = self.head(x)                                # vocabulary logits, shape (B, S, V)\n",
    "        return logits                                        # return unnormalized scores (logits)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens=100, temperature=1.0):\n",
    "        # idx: (B, S) starting prompt tokens\n",
    "        # max_new_tokens: how many tokens to append\n",
    "        # temperature: >1 makes output more random, <1 makes output more confident/peaky\n",
    "        self.eval()                                          # evaluation mode (disables dropout)\n",
    "\n",
    "        for _ in range(max_new_tokens):                      # generate one token at a time\n",
    "            idx_cond = idx[:, -self.block_size:]             # if context too long, keep only last block_size tokens\n",
    "\n",
    "            logits = self(idx_cond)                          # forward pass, shape (B, S, V)\n",
    "            next_logits = logits[:, -1, :]                   # take last position logits, shape (B, V)\n",
    "            next_logits = next_logits / temperature          # apply temperature scaling\n",
    "\n",
    "            probs = F.softmax(next_logits, dim=-1)           # convert logits -> probabilities, shape (B, V)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)# sample next token ID, shape (B, 1)\n",
    "\n",
    "            idx = torch.cat([idx, next_id], dim=1)           # append sampled token to sequence, shape (B, S+1)\n",
    "\n",
    "        return idx                                           # return full sequence (prompt + generated)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Training setup\n",
    "# ----------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # choose GPU if available, else CPU\n",
    "torch.manual_seed(0)                            # set random seed for reproducibility\n",
    "\n",
    "block_size = 64                                 # context window length (sequence length)\n",
    "model = TinyTransformerLM(                      # instantiate the model\n",
    "    vocab_size=vocab_size,                      # vocabulary size from tokenizer\n",
    "    d_model=128,                                # embedding dimension\n",
    "    nhead=4,                                    # attention heads\n",
    "    num_layers=2,                               # transformer layers\n",
    "    block_size=block_size                       # max sequence length supported\n",
    ").to(device)                                    # move model parameters to CPU/GPU\n",
    "\n",
    "opt = torch.optim.AdamW(                        # AdamW optimizer (common for Transformers)\n",
    "    model.parameters(),                         # optimize all model parameters\n",
    "    lr=3e-4,                                    # learning rate\n",
    "    weight_decay=1e-2                           # weight decay regularization\n",
    ")\n",
    "\n",
    "def lm_loss(logits, targets):\n",
    "    # logits: (B, S, V) and targets: (B, S)\n",
    "    B, S, V = logits.shape                      # unpack dimensions\n",
    "    logits_2d = logits.reshape(B*S, V)          # flatten batch+time into one dimension\n",
    "    targets_1d = targets.reshape(B*S)           # flatten targets similarly\n",
    "    return F.cross_entropy(logits_2d, targets_1d)  # standard next-token cross entropy\n",
    "\n",
    "steps = 500                                     # how many gradient updates\n",
    "batch_size = 64                                 # how many sequences per batch\n",
    "\n",
    "for step in range(1, steps + 1):                # main training loop\n",
    "    model.train()                               # training mode (enables dropout)\n",
    "\n",
    "    x, y = get_batch(                           # get a random batch\n",
    "        data=data,                              # tokenized corpus\n",
    "        batch_size=batch_size,                  # batch size\n",
    "        block_size=block_size,                  # sequence length\n",
    "        device=device                           # CPU/GPU\n",
    "    )\n",
    "\n",
    "    logits = model(x)                           # forward pass: predict logits for each position\n",
    "    loss = lm_loss(logits, y)                   # compute next-token loss\n",
    "\n",
    "    opt.zero_grad()                             # clear old gradients\n",
    "    loss.backward()                             # backprop compute gradients\n",
    "    torch.nn.utils.clip_grad_norm_(             # clip gradients to stabilize training\n",
    "        model.parameters(),                     # params to clip\n",
    "        max_norm=1.0                            # max gradient norm\n",
    "    )\n",
    "    opt.step()                                  # update parameters\n",
    "\n",
    "    if step % 100 == 0:                         # print occasionally\n",
    "        print(f\"step {step:4d} | loss {loss.item():.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Text generation demo\n",
    "# ----------------------------\n",
    "\n",
    "prompt = \"caleb \"                               # starting text prompt\n",
    "idx0 = encode(prompt).unsqueeze(0).to(device)   # encode prompt -> (S,) then add batch dim -> (1, S)\n",
    "\n",
    "out_ids = model.generate(                       # generate continuation\n",
    "    idx=idx0,                                   # starting tokens\n",
    "    max_new_tokens=10,                         # how many tokens to generate\n",
    "    temperature=1                            # sampling temperature\n",
    ")[0].cpu()                                      # take first batch element and move to CPU\n",
    "\n",
    "print(\"\\n--- generated ---\")\n",
    "print(decode(out_ids))                          # decode token IDs back to text\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
